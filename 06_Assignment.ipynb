{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simbol105/Machine-learing-assignment/blob/main/06_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the libraries"
      ],
      "metadata": {
        "id": "qzdYBBbg4QGZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "aLslTbDJzxRm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Tuple\n",
        "import torch\n",
        "from torch.nn import Module, Linear, ReLU, Softmax, Sequential, CrossEntropyLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the XOR Dataset"
      ],
      "metadata": {
        "id": "k80qMHan0c3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training set\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "\n",
        "# true labels\n",
        "y = np.array([0, 1, 1, 0])"
      ],
      "metadata": {
        "id": "qEzp5nQw0EAl"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-Hot Encoding Function"
      ],
      "metadata": {
        "id": "ShuZRSPc4N3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(y: np.ndarray, num_classes: int) -> np.ndarray:\n",
        "    \"\"\"Convert integer labels to one hot encoding.\n",
        "\n",
        "    Example: y=[1, 2], num_classes=3 --> [[0, 1, 0], [0, 0, 1]]\n",
        "\n",
        "    Args:\n",
        "        y: Input labels as integers with shape (num_datapoints)\n",
        "        num_classes: Number of possible classes\n",
        "\n",
        "    Returns:\n",
        "        One-hot encoded labels with shape (num_datapoints, num_classes)\n",
        "    \"\"\"\n",
        "    encoded = np.zeros(y.shape + (num_classes,))\n",
        "    encoded[np.arange(len(y)), y] = 1\n",
        "    return encoded"
      ],
      "metadata": {
        "id": "aRjpTMpg0gxE"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Neural Network Architectures"
      ],
      "metadata": {
        "id": "MDDB6TO93_Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_2unit_net() -> Module:\n",
        "    \"\"\"Create a two-layer MLP (1 hidden layer, 1 output layer) with 2 hidden units as described in the exercise.\n",
        "\n",
        "    Returns:\n",
        "        2-layer MLP module with 2 hidden units.\n",
        "    \"\"\"\n",
        "    # START TODO #################\n",
        "    # Define the model here\n",
        "    model = Sequential(\n",
        "        Linear(2, 2),\n",
        "        ReLU(),\n",
        "        Linear(2, 2)\n",
        "    )\n",
        "\n",
        "    # END TODO ##################\n",
        "    params = model.state_dict()\n",
        "\n",
        "    # Now we assign the model weights since we still did not learn backpropagation\n",
        "    params['0.weight'] = torch.Tensor(np.array([[3.21, 3.21], [-2.34, -2.34]]))\n",
        "    params['0.bias'] = torch.Tensor(np.array([-3.21, 2.34]))\n",
        "    params['2.weight'] = torch.Tensor(np.array([[3.19, 4.64], [-2.68, -3.44]]))\n",
        "    params['2.bias'] = torch.Tensor(np.array([-4.08, 4.42]))\n",
        "    model.load_state_dict(params)\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "xQ263clW0vyy"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_3unit_net() -> Module:\n",
        "    \"\"\"Create a two-layer MLP (1 hidden layer, 1 output layer) with 3 hidden units as described in the exercise.\n",
        "\n",
        "    Returns:\n",
        "        2-layer MLP module with 3 hidden units.\n",
        "    \"\"\"\n",
        "    # START TODO #################\n",
        "    # Define the model here\n",
        "    model = Sequential(\n",
        "        Linear(2, 3),\n",
        "        ReLU(),\n",
        "        Linear(3, 2)\n",
        "    )\n",
        "\n",
        "    # END TODO ##################\n",
        "\n",
        "    params = model.state_dict()\n",
        "    # START TODO #################\n",
        "    # change the model weights\n",
        "    params['0.weight'] = torch.Tensor(np.array([[3.21, 3.21], [-2.34, -6.34], [3.12, -0.12]]))\n",
        "    params['0.bias'] = torch.Tensor(np.array([-3.21, 2.34, 2.34]))\n",
        "    params['2.weight'] = torch.Tensor(np.array([[1.19, 8.64, 4.64], [-1.68, -3.44, -0.44]]))\n",
        "    params['2.bias'] = torch.Tensor(np.array([-20.08, 15.42]))\n",
        "\n",
        "\n",
        "\n",
        "    # END TODO ##################\n",
        "    model.load_state_dict(params)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "BNIGIa6i0yvg"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run the Model on XOR Dataset\n"
      ],
      "metadata": {
        "id": "UHAZY3ze4Gux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model_on_xor(model: Module, verbose: bool = True) -> Tuple[np.ndarray, np.float32]:\n",
        "    \"\"\"Run the XOR dataset through the model and compute the loss.\n",
        "\n",
        "    Args:\n",
        "        model: MLP to use for prediction\n",
        "        verbose: Whether to print the outputs.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "            Class predictions after softmax with shape (batch_size, num_classes)\n",
        "            Cross-Entropy loss given the model outputs and the true labels\n",
        "\n",
        "    \"\"\"\n",
        "    # Here we test if our prediction works. We first get the so-called \"logits\" (the MLP output before the softmax),\n",
        "    # then run them through the softmax function. We have to transform the prediction into one-hot format,\n",
        "    # and finally we can check whether our MLP predicts the correct values.\n",
        "\n",
        "    # START TODO #################\n",
        "    # propagate the input data (stored in the imported variable X) through the model.\n",
        "    prediction = model.forward(torch.Tensor(X))\n",
        "\n",
        "    # END TODO ##################\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Raw prediction logits:\")\n",
        "        print(prediction.detach().cpu().numpy())\n",
        "        print()\n",
        "    softmax_function = Softmax(dim=-1)\n",
        "    pred_softmax = softmax_function(prediction).cpu().detach().numpy()\n",
        "    if verbose:\n",
        "        print(\"Prediction after softmax:\")\n",
        "        print(pred_softmax)\n",
        "        print()\n",
        "\n",
        "    # START TODO #################\n",
        "    # Use the one_hot_encoding function on the labels to convert them to\n",
        "    # one-hot encoding. The labels are stored in the imported variable y.\n",
        "    Y_onehot = one_hot_encoding(y, 2)\n",
        "\n",
        "    # END TODO ##################\n",
        "\n",
        "    if verbose:\n",
        "        print(\"True labels, one-hot encoded:\")\n",
        "        print(Y_onehot)\n",
        "        print()\n",
        "\n",
        "    # Pass prediction and ground truth to the generalized Cross-Entropy Loss.\n",
        "    # Since the loss has a softmax already implemented inside of it, you need to pass the raw logits of the\n",
        "    # prediction. The loss expects one-hot encoded labels of shape (batchsize, num_classes)\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "\n",
        "    # given the true labels Y and the predictions,\n",
        "    # compute the cross entropy loss defined above\n",
        "    loss = loss_fn(prediction, torch.LongTensor(y)).cpu().detach().numpy()\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Loss:\", loss)\n",
        "\n",
        "    # return predictions and loss for testing\n",
        "    return pred_softmax, loss"
      ],
      "metadata": {
        "id": "pprNr8FY3FFo"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_test_model(model: Module) -> None:\n",
        "    \"\"\"Helper function to test if the model predicts the correct classes.\n",
        "\n",
        "    Args:\n",
        "        model: Module to predict the classes.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    pred_softmax, loss = run_model_on_xor(model, verbose=False)\n",
        "    Y_onehot = one_hot_encoding(y, 2)\n",
        "    np.testing.assert_allclose(\n",
        "        pred_softmax, Y_onehot, atol=1e-3,\n",
        "        err_msg=f\"The model predicts the wrong classes. Ground-truth: {Y_onehot}, predictions: {pred_softmax}\")\n",
        "    assert np.abs(loss) < 1e-3, f\"Loss is too high: {loss}\""
      ],
      "metadata": {
        "id": "7g965ODo3Hop"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_2unit_net()\n",
        "run_model_on_xor(model)"
      ],
      "metadata": {
        "id": "jPRQSpPc8GVD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0898f55-a50a-4c90-cbd5-b5f0a03b1ae1"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw prediction logits:\n",
            "[[ 6.7775993 -3.6295996]\n",
            " [-4.08       4.42     ]\n",
            " [-4.08       4.42     ]\n",
            " [ 6.1599007 -4.1828003]]\n",
            "\n",
            "Prediction after softmax:\n",
            "[[9.9996984e-01 3.0213278e-05]\n",
            " [2.0342699e-04 9.9979657e-01]\n",
            " [2.0342699e-04 9.9979657e-01]\n",
            " [9.9996781e-01 3.2226122e-05]]\n",
            "\n",
            "True labels, one-hot encoded:\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "\n",
            "Loss: 0.00011732114\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[9.9996984e-01, 3.0213278e-05],\n",
              "        [2.0342699e-04, 9.9979657e-01],\n",
              "        [2.0342699e-04, 9.9979657e-01],\n",
              "        [9.9996781e-01, 3.2226122e-05]], dtype=float32),\n",
              " array(0.00011732, dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_3unit_net()\n",
        "run_model_on_xor(model)"
      ],
      "metadata": {
        "id": "Xswxf9jo8J6R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb69d73f-3f0f-460d-a646-3150facf6f81"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw prediction logits:\n",
            "[[10.995199   6.3408003]\n",
            " [-9.779201  14.4432   ]\n",
            " [ 5.2544003 13.0176   ]\n",
            " [ 8.5175     7.6776   ]]\n",
            "\n",
            "Prediction after softmax:\n",
            "[[9.9057019e-01 9.4298655e-03]\n",
            " [3.0223522e-11 1.0000000e+00]\n",
            " [4.2491357e-04 9.9957508e-01]\n",
            " [6.9844419e-01 3.0155587e-01]]\n",
            "\n",
            "True labels, one-hot encoded:\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "\n",
            "Loss: 0.09219989\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[9.9057019e-01, 9.4298655e-03],\n",
              "        [3.0223522e-11, 1.0000000e+00],\n",
              "        [4.2491357e-04, 9.9957508e-01],\n",
              "        [6.9844419e-01, 3.0155587e-01]], dtype=float32),\n",
              " array(0.09219989, dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AayDtNFtsNQi"
      },
      "execution_count": 182,
      "outputs": []
    }
  ]
}